[
  {
    "url": "https://arxiv.org/abs/2507.21028",
    "title": "Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation",
    "authors": "Jiaju Chen, Yuxuan Lu, Xiaojie Wang, Huimin Zeng, Jing Huang, Jiri Gesi, Ying Xu, Bingsheng Yao, Dakuo Wang",
    "date": "[Submitted on 28 Jul 2025]",
    "abstract": "Nearly all human work is collaborative; thus, the evaluation of real-world NLP applications often requires multiple dimensions that align with diverse human perspectives. As real human evaluator resources are often scarce and costly, the emerging \"LLM-as-a-judge\" paradigm sheds light on a promising approach to leverage LLM agents to believably simulate human evaluators. Yet, to date, existing LLM-as-a-judge approaches face two limitations: persona descriptions of agents are often arbitrarily designed, and the frameworks are not generalizable to other tasks. To address these challenges, we propose MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically construct multiple evaluator personas with distinct dimensions from relevant text documents (e.g., research papers), instantiate LLM agents with the personas, and engage in-group debates with multi-agents to Generate multi-dimensional feedback. Our evaluation experiments in both the educational and medical domains demonstrate that MAJ-EVAL can generate evaluation results that better align with human experts' ratings compared with conventional automated evaluation metrics and existing LLM-as-a-judge methods.",
    "ocr_abstract": "Nearly all human work is collaborative; thus, the evaluation of real-\nworld NLP applications often requires multiple dimensions that align\nwith diverse human perspectives. As real human evaluator resources\nare often scarce and costly, the emerging \"LLM-as-a-judge\" paradigm\nsheds light on a promising approach to leverage LLM agents to\nbelievably simulate human evaluators. Yet, to date, existing LLM-as-a-\njudge approaches face two limitations: persona descriptions of agents\nare often arbitrarily designed, and the frameworks are not generalizable\nto other tasks. To address these challenges, we propose MAJ-EVAL, a\nMulti-Agent-as-Judge evaluation framework that can automatically\nconstruct multiple evaluator personas with distinct dimensions from\nrelevant text documents (e.g. research papers). instantiate LLM agents\nwith the personas, and engage in-group debates with multi-agents to\nGenerate multi-dimensional feedback. Our evaluation experiments in\nboth the educational and medical domains demonstrate that MAJ-EVAL\ncan generate evaluation results that better align with human experts’\nratings compared with conventional automated evaluation metrics and\nexisting LLM-as-a-judge methods."
  },
  {
    "url": "https://arxiv.org/abs/2507.21009",
    "title": "Memorization in Fine-Tuned Large Language Models",
    "authors": "Danil Savine, Muni Sreenivas Pydi, Jamal Atif, Olivier Cappé",
    "date": "[Submitted on 28 Jul 2025]",
    "abstract": "This study investigates the mechanisms and factors influencing memorization in fine-tuned large language models (LLMs), with a focus on the medical domain due to its privacy-sensitive nature. We examine how different aspects of the fine-tuning process affect a model's propensity to memorize training data, using the PHEE dataset of pharmacovigilance events.\nOur research employs two main approaches: a membership inference attack to detect memorized data, and a generation task with prompted prefixes to assess verbatim reproduction. We analyze the impact of adapting different weight matrices in the transformer architecture, the relationship between perplexity and memorization, and the effect of increasing the rank in low-rank adaptation (LoRA) fine-tuning.\nKey findings include: (1) Value and Output matrices contribute more significantly to memorization compared to Query and Key matrices; (2) Lower perplexity in the fine-tuned model correlates with increased memorization; (3) Higher LoRA ranks lead to increased memorization, but with diminishing returns at higher ranks.\nThese results provide insights into the trade-offs between model performance and privacy risks in fine-tuned LLMs. Our findings have implications for developing more effective and responsible strategies for adapting large language models while managing data privacy concerns.",
    "ocr_abstract": "This study investigates the mechanisms and factors influencing\nmemorization in fine-tuned large language models (LLMs), with a focus\non the medical domain due to its privacy-sensitive nature. We examine\nhow different aspects of the fine-tuning process affect a model's\npropensity to memorize training data, using the PHEE dataset of\npharmacovigilance events.\n\nOur research employs two main approaches: a membership inference\nattack to detect memorized data, and a generation task with prompted\nprefixes to assess verbatim reproduction. We analyze the impact of\nadapting different weight matrices in the transformer architecture, the\nrelationship between perplexity and memorization, and the effect of\nincreasing the rank in low-rank adaptation (LRA) fine-tuning.\n\nKey findings include: (1) Value and Output matrices contribute more\nsignificantly to memorization compared to Query and Key matrices: (2)\nLower perplexity in the fine-tuned model correlates with increased\nmemorization; (3) Higher LORA ranks lead to increased memorization,\nbut with diminishing returns at higher ranks.\n\nThese results provide insights into the trade-offs between model\nperformance and privacy risks in fine-tuned LLMs. Our findings have\nimplications for developing more effective and responsible strategies for\nadapting large language models while managing data privacy concems."
  },
  {
    "url": "https://arxiv.org/abs/2507.20956",
    "title": "Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models",
    "authors": "Max Peeperkorn, Tom Kouwenhoven, Dan Brown, Anna Jordanous",
    "date": "[Submitted on 28 Jul 2025]",
    "abstract": "Instruction-tuning large language models (LLMs) reduces the diversity of their outputs, which has implications for many tasks, particularly for creative tasks. This paper investigates the ``diversity gap'' for a writing prompt narrative generation task. This gap emerges as measured by current diversity metrics for various open-weight and open-source LLMs. The results show significant decreases in diversity due to instruction-tuning. We explore the diversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to further understand how output diversity is affected. The results indicate that DPO has the most substantial impact on diversity. Motivated by these findings, we present a new decoding strategy, conformative decoding, which guides an instruct model using its more diverse base model to reintroduce output diversity. We show that conformative decoding typically increases diversity and even maintains or improves quality.",
    "ocr_abstract": "Instruction-tuning large language models (LLMs) reduces the diversity\nof their outputs, which has implications for many tasks, particularly for\ncreative tasks. This paper investigates the “diversity gap\" for a writing\nprompt narrative generation task. This gap emerges as measured by\ncurrent diversity metrics for various open-weight and open-source\nLLMs. The results show significant decreases in diversity due to\ninstruction-tuning. We explore the diversity loss at each fine-tuning\nstage for the OLMo and OLMo 2 models to further understand how\noutput diversity is affected. The results indicate that DPO has the most\nsubstantial impact on diversity. Motivated by these findings, we present\na new decoding strategy, conformative decoding, which guides an\ninstruct model using its more diverse base model to reintroduce output\ndiversity. We show that conformative decoding typically increases\ndiversity and even maintains or improves quality."
  },
  {
    "url": "https://arxiv.org/abs/2507.20930",
    "title": "FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models",
    "authors": "Likun Tan, Kuan-Wei Huang, Kevin Wu",
    "date": "[Submitted on 28 Jul 2025]",
    "abstract": "Hallucinations in large language models pose a critical challenge for applications requiring factual reliability, particularly in high-stakes domains such as finance. This work presents an effective approach for detecting and editing factually incorrect content in model-generated responses based on the provided context. Given a user-defined domain-specific error taxonomy, we construct a synthetic dataset by inserting tagged errors into financial question-answering corpora and then fine-tune four language models, Phi-4, Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8% improvement in binary F1 score and a 30% gain in overall detection performance compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having only 4 billion parameters, maintains competitive performance with just a 2% drop in binary detection and a 0.1% decline in overall detection compared to OpenAI-o3. Our work provides a practical solution for detecting and editing factual inconsistencies in financial text generation while introducing a generalizable framework that can enhance the trustworthiness and alignment of large language models across diverse applications beyond finance. Our code and data are available at this https URL.",
    "ocr_abstract": "Hallucinations in large language models pose a critical challenge for\napplications requiring factual reliability, particularly in high-stakes\ndomains such as finance. This work presents an effective approach for\ndetecting and editing factually incorrect content in model-generated\nresponses based on the provided context. Given a user-defined\ndomain-specific error taxonomy, we construct a synthetic dataset by\ninserting tagged errors into financial question-answering corpora and\nthen fine-tune four language models, Phi-4, Phi-4-mini, Qwen3-4, and\nQwen3-14B, to detect and edit these factual inaccuracies. Our best-\nperforming model, fine-tuned Phi-4, achieves an 8% improvement in\nbinary F1 score and a 30% gain in overall detection performance\ncompared to OpenAl-o3. Notably, our fine-tuned Phi-4-mini model,\ndespite having only 4 billion parameters, maintains competitive\nperformance with just a 2% drop in binary detection and a 0.1% decline\nin overall detection compared to OpenAl-o3. Our work provides a\npractical solution for detecting and editing factual inconsistencies in\nfinancial text generation while introducing a generalizable framework\nthat can enhance the trustworthiness and alignment of large language\nmodels across diverse applications beyond finance. Our code and data\nare availahie at thic htins JR!"
  },
  {
    "url": "https://arxiv.org/abs/2507.20924",
    "title": "FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models",
    "authors": "Roberto Labadie-Tamayo, Adrian Jaques Böck, Djordje Slijepčević, Xihui Chen, Andreas Babic, Matthias Zeppelzauer",
    "date": "[Submitted on 28 Jul 2025]",
    "abstract": "Sexism has become widespread on social media and in online conversation. To help address this issue, the fifth Sexism Identification in Social Networks (EXIST) challenge is initiated at CLEF 2025. Among this year's international benchmarks, we concentrate on solving the first task aiming to identify and classify sexism in social media textual posts. In this paper, we describe our solutions and report results for three subtasks: Subtask 1.1 - Sexism Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask 1.3 - Sexism Categorization in Tweets. We implement three models to address each subtask which constitute three individual runs: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to encode input texts into a human-interpretable representation of adjectives, then used to train a lightweight classifier for downstream tasks. SCBMT extends SCBM by fusing adjective-based representation with contextual embeddings from transformers to balance interpretability and classification performance. Beyond competitive results, these two models offer fine-grained explanations at both instance (local) and class (global) levels. We also investigate how additional metadata, e.g., annotators' demographic profiles, can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data augmented with prior datasets, ranks 6th for English and Spanish and 4th for English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and Spanish and 6th for Spanish.",
    "ocr_abstract": "Sexism has become widespread on social media and in online\nconversation. To help address this issue, the fifth Sexism Identification\nin Social Networks (EXIST) challenge is initiated at CLEF 2025. Among\nthis year's international benchmarks, we concentrate on solving the first\ntask aiming to identify and classify sexism in social media textual posts.\nIn this paper, we describe our solutions and report results for three\nsubtasks: Subtask 1.1 - Sexism Identification in Tweets, Subtask 1.2 -\nSource Intention in Tweets, and Subtask 1.3 - Sexism Categorization in\nTweets. We implement three models to address each subtask which\nconstitute three individual runs: Speech Concept Bottleneck Model\n(SCBM), Speech Concept Bottleneck Model with Transformer\n(SCBMT), and a fine-tuned XLM-RoBERTa transformer model. SCBM\nuses descriptive adjectives as human-interpretable bottleneck\nconcepts. SCBM leverages large language models (LLMs) to encode\ninput texts into a human-interpretable representation of adjectives, then\nused to train a lightweight classifier for downstream tasks. SCBMT\nextends SCBM by fusing adjective-based representation with\ncontextual embeddings from transformers to balance interpretability"
  },
  {
    "url": "https://arxiv.org/abs/2507.20917",
    "title": "MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation",
    "authors": "Adrien Bazoge",
    "date": "[Submitted on 28 Jul 2025]",
    "abstract": "This work introduces MediQAl, a French medical question answering dataset designed to evaluate the capabilities of language models in factual medical recall and reasoning over real-world clinical scenarios. MediQAl contains 32,603 questions sourced from French medical examinations across 41 medical subjects. The dataset includes three tasks: (i) Multiple-Choice Question with Unique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii) Open-Ended Question with Short-Answer. Each question is labeled as Understanding or Reasoning, enabling a detailed analysis of models' cognitive capabilities. We validate the MediQAl dataset through extensive evaluation with 14 large language models, including recent reasoning-augmented models, and observe a significant performance gap between factual recall and reasoning tasks. Our evaluation provides a comprehensive benchmark for assessing language models' performance on French medical question answering, addressing a crucial gap in multilingual resources for the medical domain.",
    "ocr_abstract": "This work introduces MediQAl, a French medical question answering\ndataset designed to evaluate the capabilities of language models in\nfactual medical recall and reasoning over real-world clinical scenarios.\nMediQAl contains 32,603 questions sourced from French medical\nexaminations across 41 medical subjects. The dataset includes three\ntasks: (i) Multiple-Choice Question with Unique answer, (ii) Multiple-\nChoice Question with Multiple answer, and (iii) Open-Ended Question\nwith Short-Answer. Each question is labeled as Understanding or\nReasoning, enabling a detailed analysis of models’ cognitive\ncapabilities. We validate the Medi@Al dataset through extensive\nevaluation with 14 large language models, including recent reasoning-\naugmented models, and observe a significant performance gap\nbetween factual recall and reasoning tasks. Our evaluation provides a\ncomprehensive benchmark for assessing language models’\nperformance on French medical question answering, addressing a\ncrucial gap in multilingual resources for the medical domain."
  },
  {
    "url": "https://arxiv.org/abs/2507.20906",
    "title": "Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context Learning",
    "authors": "Jungwon Park, Wonjong Rhee",
    "date": "[Submitted on 28 Jul 2025]",
    "abstract": "In-Context Learning (ICL) enables Large Language Models (LLMs) to perform tasks by conditioning on input-output examples in the prompt, without requiring any update in model parameters. While widely adopted, it remains unclear whether prompting with multiple examples is the most effective and efficient way to convey task information. In this work, we propose Soft Injection of task embeddings. The task embeddings are constructed only once using few-shot ICL prompts and repeatedly used during inference. Soft injection is performed by softly mixing task embeddings with attention head activations using pre-optimized mixing parameters, referred to as soft head-selection parameters. This method not only allows a desired task to be performed without in-prompt demonstrations but also significantly outperforms existing ICL approaches while reducing memory usage and compute cost at inference time. An extensive evaluation is performed across 57 tasks and 12 LLMs, spanning four model families of sizes from 4B to 70B. Averaged across 57 tasks, our method outperforms 10-shot ICL by 10.1%-13.9% across 12 LLMs. Additional analyses show that our method also serves as an insightful tool for analyzing task-relevant roles of attention heads, revealing that task-relevant head positions selected by our method transfer across similar tasks but not across dissimilar ones -- underscoring the task-specific nature of head functionality. Our soft injection method opens a new paradigm for reducing prompt length and improving task performance by shifting task conditioning from the prompt space to the activation space.",
    "ocr_abstract": "In-Context Learning (ICL) enables Large Language Models (LLMs) to\nperform tasks by conditioning on input-output examples in the prompt,\nwithout requiring any update in model parameters. While widely\nadopted, it remains unclear whether prompting with multiple examples\nis the most effective and efficient way to convey task information. In this\nwork, We propose Soft Injection of task embeddings. The task\nembeddings are constructed only once using few-shot ICL prompts and\nrepeatedly used during inference. Soft injection is performed by softly\nmixing task embeddings with attention head activations using pre-\noptimized mixing parameters, referred to as soft head-selection\nparameters. This method not only allows a desired task to be\nperformed without in-prompt demonstrations but also significantly\noutperforms existing ICL approaches while reducing memory usage\nand compute cost at inference time. An extensive evaluation is\nperformed across 57 tasks and 12 LLMs, spanning four model families\nof sizes from 4B to 70B. Averaged across 57 tasks, our method\noutperforms 10-shot ICL by 10.1%-13.9% across 12 LLMs. Additional\nanalyses show that our method also serves as an insightful tool for\nanalyzing task-relevant roles of attention heads, revealing that task-\nrelevant head nositians selected hy our method transfer across <imilar"
  },
  {
    "url": "https://arxiv.org/abs/2507.20859",
    "title": "Leveraging Open-Source Large Language Models for Clinical Information Extraction in Resource-Constrained Settings",
    "authors": "Luc Builtjes, Joeran Bosma, Mathias Prokop, Bram van Ginneken, Alessa Hering",
    "date": "[Submitted on 28 Jul 2025]",
    "abstract": "Medical reports contain rich clinical information but are often unstructured and written in domain-specific language, posing challenges for information extraction. While proprietary large language models (LLMs) have shown promise in clinical natural language processing, their lack of transparency and data privacy concerns limit their utility in healthcare. This study therefore evaluates nine open-source generative LLMs on the DRAGON benchmark, which includes 28 clinical information extraction tasks in Dutch. We developed \\texttt{llm\\_extractinator}, a publicly available framework for information extraction using open-source generative LLMs, and used it to assess model performance in a zero-shot setting. Several 14 billion parameter models, Phi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved competitive results, while the bigger Llama-3.3-70B model achieved slightly higher performance at greater computational cost. Translation to English prior to inference consistently degraded performance, highlighting the need of native-language processing. These findings demonstrate that open-source LLMs, when used with our framework, offer effective, scalable, and privacy-conscious solutions for clinical information extraction in low-resource settings.",
    "ocr_abstract": "Medical reports contain rich clinical information but are often\nunstructured and written in domain-specific language, posing\nchallenges for information extraction. While proprietary large language\nmodels (LLMs) have shown promise in clinical natural language\nprocessing, their lack of transparency and data privacy concems limit\ntheir utility in healthcare. This study therefore evaluates nine open-\nsource generative LLMs on the DRAGON benchmark, which includes\n28 clinical information extraction tasks in Dutch. We developed\n\\texttt{lIm\\_extractinator}, a publicly available framework for information\nextraction using open-source generative LLMs, and used it to assess\nmodel performance in a zero-shot setting. Several 14 billion parameter\nmodels, Phi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved\ncompetitive results, while the bigger Llama-3.3-70B model achieved\nslightly higher performance at greater computational cost. Translation to\nEnglish prior to inference consistently degraded performance,\nhighlighting the need of native-language processing. These findings\ndemonstrate that open-source LLMs, when used with our framework,\noffer effective, scalable, and privacy-conscious solutions for clinical"
  },
  {
    "url": "https://arxiv.org/abs/2507.20858",
    "title": "A survey of diversity quantification in natural language processing: The why, what, where and how",
    "authors": "Louis Estève, Marie-Catherine de Marneffe, Nurit Melnik, Agata Savary, Olha Kanishcheva",
    "date": "[Submitted on 28 Jul 2025]",
    "abstract": "The concept of diversity has received increased consideration in Natural Language Processing (NLP) in recent years. This is due to various motivations like promoting and inclusion, approximating human linguistic behavior, and increasing systems' performance. Diversity has however often been addressed in an ad hoc manner in NLP, and with few explicit links to other domains where this notion is better theorized. We survey articles in the ACL Anthology from the past 6 years, with \"diversity\" or \"diverse\" in their title. We find a wide range of settings in which diversity is quantified, often highly specialized and using inconsistent terminology. We put forward a unified taxonomy of why, what on, where, and how diversity is measured in NLP. Diversity measures are cast upon a unified framework from ecology and economy (Stirling, 2007) with 3 dimensions of diversity: variety, balance and disparity. We discuss the trends which emerge due to this systematized approach. We believe that this study paves the way towards a better formalization of diversity in NLP, which should bring a better understanding of this notion and a better comparability between various approaches.",
    "ocr_abstract": "The concept of diversity has received increased consideration in\nNatural Language Processing (NLP) in recent years. This is due to\nvarious motivations like promoting and inclusion, approximating human\niinguistic behavior, and increasing systems’ performance. Diversity has\nhowever offen been addressed in an ad hoc manner in NLP, and with\nfew explicit links to other domains where this notion is better theorized,\nWe survey articles in the ACL Anthology from the past 6 years, with\n\"diversity\" or “diverse” in their title. We find a wide range of settings in\nwhich diversity is quantified, often highly specialized and using\ninconsistent terminology. We put forward a unified taxonomy of why,\nwhat on, where, and how diversity is measured in NLP. Diversity\nmeasures are cast upon a unified framework from ecology and\neconomy (Stirling, 2007) with 3 dimensions of diversity: variety, batance\nand disparity. We discuss the trends which emerge due to this\nsystematized approach. We believe that this study paves the way\ntowards a better formalization of diversity in NLP, which should bring a\nbetter understanding of this notion and a better comparability between\nvarious approaches."
  },
  {
    "url": "https://arxiv.org/abs/2507.20849",
    "title": "Latent Inter-User Difference Modeling for LLM Personalization",
    "authors": "Yilun Qiu, Tianhao Shi, Xiaoyan Zhao, Fengbin Zhu, Yang Zhang, Fuli Feng",
    "date": "[Submitted on 28 Jul 2025]",
    "abstract": "Large language models (LLMs) are increasingly integrated into users' daily lives, leading to a growing demand for personalized outputs. Previous work focuses on leveraging a user's own history, overlooking inter-user differences that are crucial for effective personalization. While recent work has attempted to model such differences, the reliance on language-based prompts often hampers the effective extraction of meaningful distinctions. To address these issues, we propose Difference-aware Embedding-based Personalization (DEP), a framework that models inter-user differences in the latent space instead of relying on language prompts. DEP constructs soft prompts by contrasting a user's embedding with those of peers who engaged with similar content, highlighting relative behavioral signals. A sparse autoencoder then filters and compresses both user-specific and difference-aware embeddings, preserving only task-relevant features before injecting them into a frozen LLM. Experiments on personalized review generation show that DEP consistently outperforms baseline methods across multiple metrics. Our code is available at this https URL.",
    "ocr_abstract": "Large language models (LLMs) are increasingly integrated into users’\ndaily lives, leading to a growing demand for personalized outputs.\nPrevious work focuses on leveraging a user's own history, overlooking\ninter-user differences that are crucial for effective personalization. While\nrecent work has attempted to model such differences, the reliance on\nlanguage-based prompts often hampers the effective extraction of\nmeaningful distinctions. To address these issues, we propose\nDifference-aware Embedding-based Personalization (DEP), a\nframework that models inter-user differences in the latent space instead\nof relying on language prompts. DEP constructs soft prompts by\ncontrasting a user's embedding with those of peers who engaged with\nsimilar content, highlighting relative behavioral signals. A sparse\nautoencoder then filters and compresses both user-specific and\ndifference-aware embeddings, preserving only task-relevant features\nbefore injecting them into a frozen LLM. Experiments on personalized\nreview generation show that DEP consistently outperforms baseline\nmethods across multiple metrics. Our code is available at this https\nURL."
  }
]